{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from helpers import * \n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = 10000\n",
    "\n",
    "x_data = np.genfromtxt('data/dataset/x_train.csv', delimiter=\",\", skip_header=1,  max_rows=MAX_ROWS)\n",
    "y_data = np.genfromtxt('data/dataset/y_train.csv', delimiter=\",\", skip_header=1,  max_rows=MAX_ROWS)\n",
    "\n",
    "x_data[np.isnan(x_data)] = 0\n",
    "x_data = normalize(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(x_data, y_data, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = x_train.shape\n",
    "initial_w = np.random.rand(D)\n",
    "\n",
    "w, loss = logistic_regression(y_train.reshape(-1, 1), x_train, initial_w.reshape(-1, 1), 10000, 0.01)\n",
    "\n",
    "pred = predict_logistic(x_test, w)\n",
    "np.sum(pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/Documents/EPFL/projects/ML_project_1/implementations.py:142: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(y * np.log(sigmoids) + (1 - y) * np.log(1 - sigmoids))\n",
      "/home/stefano/Documents/EPFL/projects/ML_project_1/implementations.py:142: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.mean(y * np.log(sigmoids) + (1 - y) * np.log(1 - sigmoids))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = x_train.shape\n",
    "initial_w = np.random.rand(D)\n",
    "\n",
    "w, loss = reg_logistic_regression(y_train.reshape(-1, 1), x_train, 0.5, initial_w.reshape(-1, 1), 1000, 0.001)\n",
    "\n",
    "pred = predict_logistic(x_test, w)\n",
    "np.sum(pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def k_fold_cross_validation(X, y, model, k, model_params):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: features, numpy array of shape (num_samples, num_features)\n",
    "    - y: targets, numpy array of shape (num_samples, )\n",
    "    - model: a classifier having fit and predict methods\n",
    "    - k: number of folds\n",
    "    - model_params: dictionary with values of model paramters\n",
    "\n",
    "    Returns:\n",
    "    - mean_accuracy: the average accuracy over the k-folds\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = num_samples // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Split data into train and test for this fold\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "        \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        # Fit model and predict\n",
    "        w, loss = model(y_train, X_train, **model_params)\n",
    "        \n",
    "        y_pred = predict_logistic(X_test, w)\n",
    "        \n",
    "        # Calculate accuracy for this fold and append to accuracies list\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    # Calculate mean accuracy over all k-folds\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(X, y, model, lambdas, gammas, model_params,  k=5):\n",
    "    \"\"\"\n",
    "    Tune hyperparameter using k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: features\n",
    "    - y: targets\n",
    "    - model_class: a class of the model that accepts the hyperparameter in its constructor\n",
    "    - param_name: name of the hyperparameter to be tuned\n",
    "    - param_values: list of values for the hyperparameter\n",
    "    - k: number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "    - best_param_value: the value of the hyperparameter that gives the best cross-validation accuracy\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_param_lambda = None\n",
    "    best_param_gamma = None\n",
    "    \n",
    "    for gamma in gammas: \n",
    "        for lambda_ in lambdas: \n",
    "\n",
    "            model_params['lambda_'] = lambda_\n",
    "            model_params['gamma'] = gamma\n",
    "            # model = model(X, y, k, **model_params)  # Construct model with the hyperparameter\n",
    "            accuracy = k_fold_cross_validation(X, y, model, k, model_params)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_param_lambda = lambda_\n",
    "                best_param_gamma = gamma\n",
    "                \n",
    "            print(f\" lambda= {lambda_} gamma= {gamma} , CV accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "    return best_param_lambda, best_param_gamma\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/Documents/EPFL/projects/ML_project_1/implementations.py:142: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(y * np.log(sigmoids) + (1 - y) * np.log(1 - sigmoids))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/Documents/EPFL/projects/ML_project_1/implementations.py:142: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.mean(y * np.log(sigmoids) + (1 - y) * np.log(1 - sigmoids))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lambda= 0.2 gamma= 0.01 , CV accuracy = 0.5470\n",
      " lambda= 0.3 gamma= 0.01 , CV accuracy = 0.5530\n",
      " lambda= 0.2 gamma= 0.05 , CV accuracy = 0.5480\n",
      " lambda= 0.3 gamma= 0.05 , CV accuracy = 0.5620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3, 0.05)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = x_train.shape\n",
    "initial_w = np.random.rand(D)\n",
    "\n",
    "# k_fold_cross_validation(x_data, y_data, logistic_regression, 5, {'initial_w': initial_w ,'max_iters': 1000, 'gamma': 0.01})\n",
    "hyperparameter_tuning(x_data, y_data, reg_logistic_regression,lambdas=[0.2, 0.3], gammas=[0.01, 0.05], model_params={'initial_w': initial_w ,'max_iters': 1000})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
